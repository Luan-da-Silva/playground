{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223d37bd-99cb-45f0-b127-6e0f9834b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_tools.setup import setup\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41330ab6-ec88-4130-bf71-7e6dc55c40d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/29 18:21:15 WARN Utils: Your hostname, luan-Dell-G15-5520 resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlp0s20f3)\n",
      "23/10/29 18:21:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/10/29 18:21:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/29 18:21:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/29 18:21:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/10/29 18:21:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/10/29 18:21:16 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as \"spark\"\n"
     ]
    }
   ],
   "source": [
    "spark = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521c53d9-f73e-47c9-bb5b-1527bd0d4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"SELECT * FROM film\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf7b33f-dde6-435b-be28-daf002bd418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+--------+\n",
      "|              title|length|duration|\n",
      "+-------------------+------+--------+\n",
      "|   Academy Dinosaur|    86|  Medium|\n",
      "|     Ace Goldfinger|    48|   Short|\n",
      "|   Adaptation Holes|    50|   Short|\n",
      "|   Affair Prejudice|   117|  Medium|\n",
      "|        African Egg|   130|    Long|\n",
      "|       Agent Truman|   169|    Long|\n",
      "|    Airplane Sierra|    62|  Medium|\n",
      "|    Airport Pollock|    54|  Medium|\n",
      "|      Alabama Devil|   114|  Medium|\n",
      "|   Aladdin Calendar|    63|  Medium|\n",
      "|    Alamo Videotape|   126|    Long|\n",
      "|     Alaska Phantom|   136|    Long|\n",
      "|        Ali Forever|   150|    Long|\n",
      "|     Alice Fantasia|    94|  Medium|\n",
      "|       Alien Center|    46|   Short|\n",
      "|    Alley Evolution|   180|    Long|\n",
      "|         Alone Trip|    82|  Medium|\n",
      "|      Alter Victory|    57|  Medium|\n",
      "|       Amadeus Holy|   113|  Medium|\n",
      "|Amelie Hellfighters|    79|  Medium|\n",
      "+-------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('title',\n",
    "          'length',\n",
    "          F.when(\n",
    "              (F.col('length')>0)\n",
    "              & (F.col('length')<=50),'Short'\n",
    "          )\\\n",
    "          .when((F.col('length')>50) \n",
    "              & (F.col('length')<=120),'Medium'\n",
    "          ).otherwise('Long').alias('duration')\n",
    "         ).orderBy('title').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17acf73b-e1ae-446f-b8cd-ad6ad731232d",
   "metadata": {},
   "source": [
    "# CASE WHEN with aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e19b21-ba34-4b18-ae70-d63fcecb71c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+\n",
      "|Economy|Mass|Premium|\n",
      "+-------+----+-------+\n",
      "|    341| 323|    336|\n",
      "+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.sum(F.when(F.col('rental_rate') == 0.99,1).otherwise(0)).alias('Economy'),\n",
    "          F.sum(F.when(F.col('rental_rate') == 2.99,1).otherwise(0)).alias('Mass'),\n",
    "          F.sum(F.when(F.col('rental_rate') == 4.99,1).otherwise(0)).alias('Premium')).show()\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eaddff1-0e2c-4317-afb0-c6499a0513d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+--------------------+\n",
      "|              title|rating|  rating_description|\n",
      "+-------------------+------+--------------------+\n",
      "|   Academy Dinosaur|    PG|Parental Guidance...|\n",
      "|     Ace Goldfinger|     G|   General Audiences|\n",
      "|   Adaptation Holes| NC-17|         Adults Only|\n",
      "|   Affair Prejudice|     G|   General Audiences|\n",
      "|        African Egg|     G|   General Audiences|\n",
      "|       Agent Truman|    PG|Parental Guidance...|\n",
      "|    Airplane Sierra| PG-13|Parents Strongly ...|\n",
      "|    Airport Pollock|     R|          Restricted|\n",
      "|      Alabama Devil| PG-13|Parents Strongly ...|\n",
      "|   Aladdin Calendar| NC-17|         Adults Only|\n",
      "|    Alamo Videotape|     G|   General Audiences|\n",
      "|     Alaska Phantom|    PG|Parental Guidance...|\n",
      "|        Ali Forever|    PG|Parental Guidance...|\n",
      "|     Alice Fantasia| NC-17|         Adults Only|\n",
      "|       Alien Center| NC-17|         Adults Only|\n",
      "|    Alley Evolution| NC-17|         Adults Only|\n",
      "|         Alone Trip|     R|          Restricted|\n",
      "|      Alter Victory| PG-13|Parents Strongly ...|\n",
      "|       Amadeus Holy|    PG|Parental Guidance...|\n",
      "|Amelie Hellfighters|     R|          Restricted|\n",
      "+-------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('title',\n",
    "          'rating',\n",
    "          F.when(F.col('rating') == 'G','General Audiences')\\\n",
    "          .when(F.col('rating') == 'PG','Parental Guidance Suggested')\\\n",
    "          .when(F.col('rating') == 'PG-13','Parents Strongly Cautioned')\\\n",
    "          .when(F.col('rating') == 'R', 'Restricted')\\\n",
    "          .when(F.col('rating') == 'NC-17', 'Adults Only').alias('rating_description')\n",
    "         ).orderBy('title').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f369f0-51c7-4f62-bb7a-506672463db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------------+--------------------------+----------+-----------+\n",
      "|General_Audiences|Parental_Guidance_Suggested|Parents_Strongly_Cautioned|Restricted|Adults Only|\n",
      "+-----------------+---------------------------+--------------------------+----------+-----------+\n",
      "|              178|                        194|                       223|       195|        210|\n",
      "+-----------------+---------------------------+--------------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.sum(F.when(F.col('rating') == 'G',1).otherwise(0)).alias('General_Audiences'),\n",
    "          F.sum(F.when(F.col('rating') == 'PG',1).otherwise(0)).alias('Parental_Guidance_Suggested'),\n",
    "          F.sum(F.when(F.col('rating') == 'PG-13',1).otherwise(0)).alias('Parents_Strongly_Cautioned'),\n",
    "          F.sum(F.when(F.col('rating') == 'R',1).otherwise(0)).alias('Restricted'),\n",
    "          F.sum(F.when(F.col('rating') == 'NC-17',1).otherwise(0)).alias('Adults Only')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c56d8-13ec-42a1-b258-750d3407dddb",
   "metadata": {},
   "source": [
    "# COALESCE: not work as you expect that does"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eec253-ebbe-4fe7-b5a6-56e797c438eb",
   "metadata": {},
   "source": [
    "## From pyspark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "baf51748-be16-401f-9f47-131bd4ca1b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|coalesce(a, b)|\n",
      "+--------------+\n",
      "|          null|\n",
      "|             1|\n",
      "|             2|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    "cDf.select(F.coalesce(cDf[\"a\"], cDf[\"b\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b860305-4366-4051-b929-592f9d225601",
   "metadata": {},
   "source": [
    "# IFNULL (Pyspark 3.5.0 and on exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e3b80dd-857a-46f9-b4d6-359243021b31",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'ifnull'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(\u001b[38;5;28;01mNone\u001b[39;00m,), (\u001b[38;5;241m1\u001b[39m,)], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mifnull\u001b[49m(df\u001b[38;5;241m.\u001b[39me, sf\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m8\u001b[39m)))\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.functions' has no attribute 'ifnull'"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n",
    "df.select(F.ifnull(df.e, sf.lit(8))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a8525-4ee2-409f-9152-fa2fc1e25fc2",
   "metadata": {},
   "source": [
    "# NULLIF (Pyspark 3.5.0 and on exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "883cfba6-fa2f-4272-815c-1e0d6423ec8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'nullif'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9\u001b[39m,)], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnullif\u001b[49m(df\u001b[38;5;241m.\u001b[39ma, df\u001b[38;5;241m.\u001b[39mb)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.functions' has no attribute 'nullif'"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(None, None,), (1, 9,)], [\"a\", \"b\"])\n",
    "df.select(F.nullif(df.a, df.b).alias('r')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2be13-66b7-486b-9064-e98790e8c907",
   "metadata": {},
   "source": [
    "# CAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3783a67c-b5f2-444b-8fcb-2b01ccb56a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------------------+\n",
      "|rental_id|rental_date_as_date|rental_date_as_timestamp|\n",
      "+---------+-------------------+------------------------+\n",
      "|        2|         2005-05-24|     2005-05-24 22:54:33|\n",
      "|        3|         2005-05-24|     2005-05-24 23:03:39|\n",
      "|        4|         2005-05-24|     2005-05-24 23:04:41|\n",
      "|        5|         2005-05-24|     2005-05-24 23:05:21|\n",
      "|        6|         2005-05-24|     2005-05-24 23:08:07|\n",
      "|        7|         2005-05-24|     2005-05-24 23:11:53|\n",
      "|        8|         2005-05-24|     2005-05-24 23:31:46|\n",
      "|        9|         2005-05-25|     2005-05-25 00:00:40|\n",
      "|       10|         2005-05-25|     2005-05-25 00:02:21|\n",
      "|       11|         2005-05-25|     2005-05-25 00:09:02|\n",
      "|       12|         2005-05-25|     2005-05-25 00:19:27|\n",
      "|       13|         2005-05-25|     2005-05-25 00:22:55|\n",
      "|       14|         2005-05-25|     2005-05-25 00:31:15|\n",
      "|       15|         2005-05-25|     2005-05-25 00:39:22|\n",
      "|       16|         2005-05-25|     2005-05-25 00:43:11|\n",
      "|       17|         2005-05-25|     2005-05-25 01:06:36|\n",
      "|       18|         2005-05-25|     2005-05-25 01:10:47|\n",
      "|       19|         2005-05-25|     2005-05-25 01:17:24|\n",
      "|       20|         2005-05-25|     2005-05-25 01:48:41|\n",
      "|       21|         2005-05-25|     2005-05-25 01:59:46|\n",
      "+---------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"SELECT * FROM rental\"\"\")\n",
    "df.select('rental_id',\n",
    "          F.col('rental_date').cast('date').alias('rental_date_as_date'),\n",
    "          F.col('rental_date').cast('timestamp').alias('rental_date_as_timestamp')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
